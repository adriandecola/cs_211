This project dealt with fetching data online, parsing through HTML and JSON files, writing to files, and using APIs. 

Purpose of Part 1:
               To use web scraping to gather and manipulate information from
               various websites. This includes making a dictionary of all
               the states and their capitals via scraping from wikipedia,
               writing the dictionary to a file, then finding the latitude
               and longitude of each capital including that in the dictionary
               and then writing that to another file, and then presenting minimum
               and maximum earthquakes between a given radius of each state
               capital. 
Purpose of Part 2:
               To use web scraping to gather and manipulate information about earthquake data.
               In particular, the the program can display information on earthquakes within
               a given time frame and above a certain magnitude, within a given time frame
               and radius of a particular latitude-longitude coordinate, and withing a given
               time frame and radius of a particular address. 
               
All of these file's functionality can be tested by running them in any Python enviroment. Some non-standard python libraries may need to be downloaded including: requests, datetime, and geopy.

Try them out!
